{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "545f994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for all parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06099153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29cf0bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the concrete data as we did in the previous lab\n",
    "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
    "\n",
    "# print out to see if it is correct\n",
    "concrete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59afcad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the target value and predictors seperately\n",
    "\n",
    "# get all column names\n",
    "columns = concrete_data.columns\n",
    "\n",
    "# get others except Strength which will be the target value\n",
    "columns_except_target = columns[columns != \"Strength\"]\n",
    "\n",
    "predictors = concrete_data[columns_except_target]\n",
    "\n",
    "target = concrete_data[\"Strength\"]\n",
    "\n",
    "# print them to see if they are correct\n",
    "#predictors.head()\n",
    "#target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "594bd71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our regression model with one hidden layer for part A, B and C\n",
    "def regression_model(size):\n",
    "    # create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add one hidden layer with 10 nodes, relu as activation function and input_shape as the number of columns\n",
    "    model.add(Dense(10, activation='relu', input_shape=(predictors.shape[1],)))\n",
    "\n",
    "    # for part D, to do 3 hidden layers \n",
    "    if size == 3:\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "                  \n",
    "    # add output layer with one output node\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model with adam as optimizer and mean_squared_error as loss function\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8ac3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fuction which does training and testing for 50 times and save the error between \n",
    "# predictions and the actual values of strength in an array\n",
    "\n",
    "def training_and_testing(_predictors, epoch, modelSize):\n",
    "    errors = np.array([0 for i in range(50)])\n",
    "\n",
    "    for i in range(50):\n",
    "        # split dataset into test and train data as test will be 30% of the whole dataset\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_predictors, target, test_size=0.3)\n",
    "        # train the model\n",
    "\n",
    "        # create model if modelSize is 3 then we are creating model for part D, else for other parts\n",
    "        if modelSize == 3:\n",
    "            model = regression_model(3)\n",
    "        else:\n",
    "            model = regression_model(1)\n",
    "            \n",
    "        # train with given number of epochs, epoch, and 0.3 portion of data as validation data\n",
    "        model.fit(x_train, y_train, validation_split=0.3, epochs=epoch, verbose=0)\n",
    "\n",
    "        # evaluate the model with test data and get the mean_squared_error of y_test and the predictions\n",
    "        score = model.evaluate(x_test, y_test)\n",
    "        \n",
    "        # put score into errors\n",
    "        errors[i] = score\n",
    "\n",
    "    mean_value_of_errors = np.mean(errors)\n",
    "    std_value_of_errors = np.std(errors)\n",
    "\n",
    "    print(\"Mean value: \", mean_value_of_errors, \" Standard deviation: \", std_value_of_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ecabe58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 336.0016\n",
      "10/10 [==============================] - 0s 998us/step - loss: 700.5020\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 310.6544\n",
      "10/10 [==============================] - 0s 972us/step - loss: 563.9411\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 650.3973\n",
      "10/10 [==============================] - 0s 665us/step - loss: 626.9977\n",
      "10/10 [==============================] - 0s 616us/step - loss: 271.9106\n",
      "10/10 [==============================] - 0s 811us/step - loss: 157.4043\n",
      "10/10 [==============================] - 0s 598us/step - loss: 295.9262\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 110.1079\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 114.9627\n",
      "10/10 [==============================] - 0s 864us/step - loss: 1475.1982\n",
      "10/10 [==============================] - 0s 930us/step - loss: 597.9677\n",
      "10/10 [==============================] - 0s 650us/step - loss: 122.0405\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 319.1910\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 204.8340\n",
      "10/10 [==============================] - 0s 1000us/step - loss: 149.6307\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 122.6432\n",
      "10/10 [==============================] - 0s 991us/step - loss: 137.2166\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 185.6846\n",
      "10/10 [==============================] - 0s 854us/step - loss: 100.9563\n",
      "10/10 [==============================] - 0s 1000us/step - loss: 729.7560\n",
      "10/10 [==============================] - 0s 991us/step - loss: 176.7818\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 440.8413\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 837.9318\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 139.4066\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 4942.3911\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1731.8895\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 128.3978\n",
      "10/10 [==============================] - 0s 887us/step - loss: 128.1412\n",
      "10/10 [==============================] - 0s 972us/step - loss: 140.2073\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 174.1240\n",
      "10/10 [==============================] - 0s 887us/step - loss: 168.6625\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 252.3246\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 88.7337\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 3428.4089\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 116.3073\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 620.8991\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 440.6214\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 354.3537\n",
      "10/10 [==============================] - 0s 0s/step - loss: 265.6483\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 206.9328\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 123.3842\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 118.1968\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 291.9080\n",
      "10/10 [==============================] - 0s 898us/step - loss: 1376.0823\n",
      "10/10 [==============================] - 0s 941us/step - loss: 280.1237\n",
      "10/10 [==============================] - 0s 938us/step - loss: 112.2177\n",
      "10/10 [==============================] - 0s 860us/step - loss: 903.7493\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 527.1190\n",
      "Mean value:  535.46  Standard deviation:  840.768843618744\n"
     ]
    }
   ],
   "source": [
    "# PART A\n",
    "\n",
    "# run training and testing with 50 epochs\n",
    "training_and_testing(predictors, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6833cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac741750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 986us/step - loss: 752.2472\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 546.5400\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 555.8629\n",
      "10/10 [==============================] - 0s 992us/step - loss: 781.8220\n",
      "10/10 [==============================] - 0s 973us/step - loss: 569.0255\n",
      "10/10 [==============================] - 0s 945us/step - loss: 899.7490\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 685.9031\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 613.6836\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 507.3885\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 635.9065\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 752.7905\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 499.1844\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 580.0135\n",
      "10/10 [==============================] - 0s 884us/step - loss: 602.9120\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 634.6255\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 789.9987\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 585.0245\n",
      "10/10 [==============================] - 0s 698us/step - loss: 907.1504\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 751.7791\n",
      "10/10 [==============================] - 0s 577us/step - loss: 561.7179\n",
      "10/10 [==============================] - 0s 943us/step - loss: 738.8185\n",
      "10/10 [==============================] - 0s 609us/step - loss: 601.0665\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 685.0549\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 861.2505\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 535.5732\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 882.6033\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 571.9286\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 687.6118\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 625.6924\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 511.0997\n",
      "10/10 [==============================] - 0s 987us/step - loss: 563.4871\n",
      "10/10 [==============================] - 0s 945us/step - loss: 851.9189\n",
      "10/10 [==============================] - 0s 763us/step - loss: 533.1287\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 606.5718\n",
      "10/10 [==============================] - 0s 927us/step - loss: 670.8266\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 747.1047\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 648.4272\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 594.5458\n",
      "10/10 [==============================] - 0s 746us/step - loss: 758.4180\n",
      "10/10 [==============================] - 0s 912us/step - loss: 599.2404\n",
      "10/10 [==============================] - 0s 884us/step - loss: 550.5134\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 544.6740\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 689.9737\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 880.7391\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 692.6329\n",
      "10/10 [==============================] - 0s 999us/step - loss: 614.1363\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 558.5912\n",
      "10/10 [==============================] - 0s 899us/step - loss: 685.0179\n",
      "10/10 [==============================] - 0s 948us/step - loss: 716.1576\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 782.7616\n",
      "Mean value:  663.54  Standard deviation:  112.2445918519017\n"
     ]
    }
   ],
   "source": [
    "# normalize data\n",
    "normalized_predictors = (predictors - predictors.mean())/predictors.std()\n",
    "\n",
    "# do the same training in Part A and testing with normalized data\n",
    "training_and_testing(normalized_predictors, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03aa5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In both part A and part B the mean values are close to each other but in part A since we dont have normalization \n",
    "# the standard deviation is high we can see that data are distributed far from the mean but in part B since we have\n",
    "# normalization the standart deviation is dramatically decreased which we can derive that now our data distribution is\n",
    "# better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c9f0f32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 237.9739\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 206.5623\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 213.3694\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 219.2961\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 285.3963\n",
      "10/10 [==============================] - 0s 603us/step - loss: 206.9688\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 213.8887\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 172.7236\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 247.0452\n",
      "10/10 [==============================] - 0s 992us/step - loss: 202.4085\n",
      "10/10 [==============================] - 0s 984us/step - loss: 234.6082\n",
      "10/10 [==============================] - 0s 972us/step - loss: 230.4154\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 200.4816\n",
      "10/10 [==============================] - 0s 457us/step - loss: 250.2699\n",
      "10/10 [==============================] - 0s 903us/step - loss: 197.9239\n",
      "10/10 [==============================] - 0s 539us/step - loss: 213.4297\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 248.8183\n",
      "10/10 [==============================] - 0s 907us/step - loss: 248.0726\n",
      "10/10 [==============================] - 0s 998us/step - loss: 205.2124\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 207.0778\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 186.9226\n",
      "10/10 [==============================] - 0s 780us/step - loss: 222.5930\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 248.5146\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 204.2220\n",
      "10/10 [==============================] - 0s 504us/step - loss: 179.4813\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 229.2164\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 323.0098\n",
      "10/10 [==============================] - 0s 836us/step - loss: 180.1163\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 187.8055\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 183.4449\n",
      "10/10 [==============================] - 0s 891us/step - loss: 200.1567\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 275.2249\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 197.0406\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 253.1616\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 336.2224\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 184.7681\n",
      "10/10 [==============================] - 0s 924us/step - loss: 221.8358\n",
      "10/10 [==============================] - 0s 992us/step - loss: 238.2509\n",
      "10/10 [==============================] - 0s 994us/step - loss: 316.8114\n",
      "10/10 [==============================] - 0s 919us/step - loss: 198.6124\n",
      "10/10 [==============================] - 0s 969us/step - loss: 213.4347\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 188.6575\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 263.0505\n",
      "10/10 [==============================] - 0s 618us/step - loss: 221.1997\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 211.2155\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 288.4905\n",
      "10/10 [==============================] - 0s 816us/step - loss: 196.8638\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 253.9624\n",
      "10/10 [==============================] - 0s 998us/step - loss: 190.9736\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 219.1262\n",
      "Mean value:  224.66  Standard deviation:  37.43132912414412\n"
     ]
    }
   ],
   "source": [
    "# PART C\n",
    "\n",
    "# do the same training in Part B with 100 epochs\n",
    "training_and_testing(normalized_predictors, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ce6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In part C we increased the number of epochs meaning that we trained our model twice more than in part B and that has\n",
    "# positive effect since the model learned better we have low mean value of error and since we have normalized data we have\n",
    "# relatively low value of standard deviation  as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e326869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 145.7346\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 136.3754\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 159.2556\n",
      "10/10 [==============================] - 0s 906us/step - loss: 168.7769\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 156.8856\n",
      "10/10 [==============================] - 0s 985us/step - loss: 139.7247\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 137.6386\n",
      "10/10 [==============================] - 0s 871us/step - loss: 152.9378\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 162.3492\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 156.9148\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 149.7738\n",
      "10/10 [==============================] - 0s 997us/step - loss: 131.6630\n",
      "10/10 [==============================] - 0s 786us/step - loss: 155.9235\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 167.7863\n",
      "10/10 [==============================] - 0s 855us/step - loss: 131.1960\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 137.5447\n",
      "10/10 [==============================] - 0s 972us/step - loss: 138.5306\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 140.2834\n",
      "10/10 [==============================] - 0s 997us/step - loss: 165.2448\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 150.0943\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 146.9640\n",
      "10/10 [==============================] - 0s 770us/step - loss: 143.9749\n",
      "10/10 [==============================] - 0s 861us/step - loss: 152.1854\n",
      "10/10 [==============================] - 0s 809us/step - loss: 146.3109\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 150.5298\n",
      "10/10 [==============================] - 0s 984us/step - loss: 170.0521\n",
      "10/10 [==============================] - 0s 996us/step - loss: 147.3461\n",
      "10/10 [==============================] - 0s 776us/step - loss: 153.6538\n",
      "10/10 [==============================] - 0s 997us/step - loss: 149.4426\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 115.2315\n",
      "10/10 [==============================] - 0s 965us/step - loss: 163.4275\n",
      "10/10 [==============================] - 0s 983us/step - loss: 141.3574\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 186.5821\n",
      "10/10 [==============================] - 0s 998us/step - loss: 121.8840\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 160.5764\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 142.8210\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 163.6419\n",
      "10/10 [==============================] - 0s 999us/step - loss: 175.2667\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 148.6547\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 174.3229\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 158.2112\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 179.9891\n",
      "10/10 [==============================] - 0s 890us/step - loss: 147.9767\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 164.1688\n",
      "10/10 [==============================] - 0s 952us/step - loss: 178.8689\n",
      "10/10 [==============================] - 0s 897us/step - loss: 144.4199\n",
      "10/10 [==============================] - 0s 931us/step - loss: 150.0901\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 181.9238\n",
      "10/10 [==============================] - 0s 998us/step - loss: 138.3289\n",
      "10/10 [==============================] - 0s 998us/step - loss: 166.0838\n",
      "Mean value:  152.44  Standard deviation:  15.138242962774777\n"
     ]
    }
   ],
   "source": [
    "# PART D\n",
    "\n",
    "# do the same training in Part B with three hidden layered network\n",
    "training_and_testing(normalized_predictors, 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14289440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part D is the one we get the best result since we have 3 hidden layered network in here with 50 epochs and normalized data\n",
    "# we can drive that for this dataset more complex network worked better than we had in part B because that is the only\n",
    "# difference we have from part B. We can see mean value is the minimum in here with 152.44 and deviation is also the minimum\n",
    "# among others with value approximately 15.14 which means we have calculated error in each step and we find close values in each"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
